{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1/fvv6Kof9zn0gZwx12GD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codedm24/Transformers/blob/Transformers-Intro/Multiple-Sequences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1E1tcGY12BdX",
        "outputId": "eeb01cbe-d8e3-4830-967c-dbf3aa83574d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers"
      ],
      "metadata": {
        "id": "ljgutrAt2uIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[sentencepiece]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jwssKUd2zyf",
        "outputId": "4e223d50-7fad-47a1-ded3-b591f9a48b11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (4.67.1)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (4.25.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers[sentencepiece]) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers[sentencepiece]) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-G8L1F5zsag",
        "outputId": "a8576d37-d0d1-43f2-dd4d-a3c73a131fee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens: ['what', 'does', 'machine', 'learning', 'do', '?']\n",
            "ids: [2054, 2515, 3698, 4083, 2079, 1029]\n",
            "Input IDs: tensor([[2054, 2515, 3698, 4083, 2079, 1029]])\n",
            "Logits: tensor([[ 3.5672, -2.8648]], grad_fn=<AddmmBackward0>)\n",
            "what does machine learning do?\n",
            "what does machine learning do?\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"What does machine learning do?\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "print(f\"tokens: {tokens}\")\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(f\"ids: {ids}\")\n",
        "\n",
        "input_ids = torch.tensor([ids])\n",
        "print(\"Input IDs:\", input_ids)\n",
        "\n",
        "output = model(input_ids)\n",
        "print(\"Logits:\", output.logits)\n",
        "\n",
        "print(tokenizer.decode(ids))\n",
        "print(tokenizer.decode(input_ids[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Padding sequence ids"
      ],
      "metadata": {
        "id": "-k9t8gsyg3sV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence1_ids = [[200,200,200]]\n",
        "sequence2_ids = [[200,200]]\n",
        "batched_ids = [\n",
        "    [200,200,200],\n",
        "    [200,200,tokenizer.pad_token_id]\n",
        "]\n"
      ],
      "metadata": {
        "id": "zek2drCNg6DX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Padding the inputs"
      ],
      "metadata": {
        "id": "MOqT6ATA_KyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "sequence_ids1 = [[200,200,200]]\n",
        "sequence_ids2 = [[200,200]]\n",
        "batched_ids = [\n",
        "    [200,200,200],\n",
        "    [200,200,tokenizer.pad_token_id]\n",
        "]\n",
        "print(model(torch.tensor(sequence_ids1)))\n",
        "print(model(torch.tensor(sequence_ids2)))\n",
        "print(model(torch.tensor(batched_ids)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xooRsOFA_Ml_",
        "outputId": "a83d33ee-d6f5-4509-c447-b7791fda1ed1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.5694, -1.3895],\n",
            "        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Padding the inputs with attention mask"
      ],
      "metadata": {
        "id": "zYXu1L55Bb4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "sequence_ids1 = [[200,200,200]]\n",
        "sequence_ids2 = [[200,200]]\n",
        "batched_ids = [\n",
        "    [200,200,200],\n",
        "    [200,200,tokenizer.pad_token_id]\n",
        "]\n",
        "\n",
        "attention_mask = [\n",
        "    [1,1,1],\n",
        "    [1,1,0]\n",
        "]\n",
        "\n",
        "print(torch.tensor(sequence_ids1))\n",
        "print(torch.tensor(sequence_ids2))\n",
        "print(torch.tensor(batched_ids))\n",
        "print(torch.tensor(attention_mask))\n",
        "\n",
        "print(model(torch.tensor(sequence_ids1)))\n",
        "print(model(torch.tensor(sequence_ids2)))\n",
        "print(model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask)))\n",
        "\n",
        "\n",
        "#print(tokenizer.decode(torch.tensor(sequence_ids1)))\n",
        "#print(tokenizer.decode(sequence_ids1[\"input_ids\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKjSViIRBfXN",
        "outputId": "b9c9b5a3-d784-4ece-e74a-ab098ff582bf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[200, 200, 200]])\n",
            "tensor([[200, 200]])\n",
            "tensor([[200, 200, 200],\n",
            "        [200, 200,   0]])\n",
            "tensor([[1, 1, 1],\n",
            "        [1, 1, 0]])\n",
            "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.5694, -1.3895],\n",
            "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Tokenizer"
      ],
      "metadata": {
        "id": "krul9ziCOe8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = \"What does Machine Learning do?\"\n",
        "model_inputs = tokenizer(sequence)\n",
        "print(f\"model inputs: {model_inputs}\")\n",
        "print(f\"model inputs: {model_inputs['input_ids']}\")\n",
        "ids = tokenizer.convert_tokens_to_ids(model_inputs)\n",
        "print(f\"ids: {ids}\")\n",
        "print(torch.tensor(model_inputs['input_ids']))\n",
        "print(tokenizer.decode(model_inputs['input_ids']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOGb2_3OOhaH",
        "outputId": "17f334fe-7545-4a32-cf8c-df22848b857f"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model inputs: {'input_ids': [101, 2054, 2515, 3698, 4083, 2079, 1029, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "model inputs: [101, 2054, 2515, 3698, 4083, 2079, 1029, 102]\n",
            "ids: [100, 100]\n",
            "tensor([ 101, 2054, 2515, 3698, 4083, 2079, 1029,  102])\n",
            "[CLS] what does machine learning do? [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = [\"What does Machine Learning do?\"]\n",
        "model_inputs = tokenizer(sequence, padding=True, return_tensors=\"pt\")\n",
        "print(f\"model inputs: {model_inputs}\")\n",
        "print(f\"model inputs: {model_inputs['input_ids']}\")\n",
        "ids = tokenizer.convert_tokens_to_ids(model_inputs)\n",
        "print(f\"ids: {ids}\")\n",
        "print(torch.tensor(model_inputs['input_ids']))\n",
        "print(tokenizer.decode(model_inputs['input_ids'][0]))"
      ],
      "metadata": {
        "id": "gzvJ17RLgyEc",
        "outputId": "8f8c4d07-9048-4c85-da08-6f5222edd78b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model inputs: {'input_ids': tensor([[ 101, 2054, 2515, 3698, 4083, 2079, 1029,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "model inputs: tensor([[ 101, 2054, 2515, 3698, 4083, 2079, 1029,  102]])\n",
            "ids: [100, 100]\n",
            "tensor([[ 101, 2054, 2515, 3698, 4083, 2079, 1029,  102]])\n",
            "[CLS] what does machine learning do? [SEP]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-50-658768a17e53>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  print(torch.tensor(model_inputs['input_ids']))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Tokenizer parameters - padding, max length"
      ],
      "metadata": {
        "id": "jjn-SXLWZtrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "sequence = \"What are the branches of Machine Learning?\"\n",
        "model_inputs = tokenizer(sequence)\n",
        "print(f\"model_inputs: {model_inputs}\")\n",
        "\n",
        "# pad sequences to max sequqnece length\n",
        "model_inputs1 = tokenizer(sequence, padding='longest')\n",
        "print(f\"model_inputs1: {model_inputs1}\")\n",
        "\n",
        "#pad sequences to model max length\n",
        "model_inputs2 = tokenizer(sequence, padding='max_length')\n",
        "print(f\"model_inputs2: {model_inputs2}\")\n",
        "\n",
        "#pad sequences to the specified max length\n",
        "model_inputs3 = tokenizer(sequence, padding='max_length', max_length=10)\n",
        "print(f\"model_inputs3: {model_inputs3}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuxHROaDUFsO",
        "outputId": "19a06459-eab6-457f-a74e-d2ce9b260ce7"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_inputs: {'input_ids': [101, 2054, 2024, 1996, 5628, 1997, 3698, 4083, 1029, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "model_inputs1: {'input_ids': [101, 2054, 2024, 1996, 5628, 1997, 3698, 4083, 1029, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "model_inputs2: {'input_ids': [101, 2054, 2024, 1996, 5628, 1997, 3698, 4083, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
            "model_inputs3: {'input_ids': [101, 2054, 2024, 1996, 5628, 1997, 3698, 4083, 1029, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Truncation using Tokenizer"
      ],
      "metadata": {
        "id": "e1EYkGHtZEFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "sequence = \"What are the branches of Machine Learning?, How to learn\"\n",
        "\n",
        "#truncate sequence longer than model max length\n",
        "model_inputs1 = tokenizer(sequence, truncation=True)\n",
        "print(f\"model_inputs1: {model_inputs1}\")\n",
        "\n",
        "#truncate sequence longer than the specified max length\n",
        "model_inputs2 = tokenizer(sequence, max_length=10, truncation=True)\n",
        "print(f\"model_inputs2: {model_inputs2}\")"
      ],
      "metadata": {
        "id": "d4IgBT_zYSyi",
        "outputId": "264d061c-a528-4242-b692-430e17e926e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_inputs1: {'input_ids': [101, 2054, 2024, 1996, 5628, 1997, 3698, 4083, 1029, 1010, 2129, 2000, 4553, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "model_inputs2: {'input_ids': [101, 2054, 2024, 1996, 5628, 1997, 3698, 4083, 1029, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Tokenizer - converting specific framework tensors"
      ],
      "metadata": {
        "id": "sXkVWHPWb9Iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "sequence = \"What are the branches of Machine Learning? How to learn\"\n",
        "\n",
        "#return pytorch tensors\n",
        "model_inputs1 = tokenizer(sequence, padding=True, return_tensors=\"pt\")\n",
        "print(f\"model_inputs1: {model_inputs1}\")\n",
        "\n",
        "#return tensorflow tensors\n",
        "model_inputs2 = tokenizer(sequence, padding=True, return_tensors=\"tf\")\n",
        "print(f\"model_inputs2: {model_inputs2}\")\n",
        "\n",
        "#return numpy array\n",
        "model_input3 = tokenizer(sequence, padding=True, return_tensors=\"np\")\n",
        "print(f\"model_input3: {model_input3}\")"
      ],
      "metadata": {
        "id": "i_rZuxICcI2o",
        "outputId": "00f55e54-3c67-4808-fb7c-39f46597876c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_inputs1: {'input_ids': tensor([[ 101, 2054, 2024, 1996, 5628, 1997, 3698, 4083, 1029, 2129, 2000, 4553,\n",
            "          102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "model_inputs2: {'input_ids': <tf.Tensor: shape=(1, 13), dtype=int32, numpy=\n",
            "array([[ 101, 2054, 2024, 1996, 5628, 1997, 3698, 4083, 1029, 2129, 2000,\n",
            "        4553,  102]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 13), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
            "model_input3: {'input_ids': array([[ 101, 2054, 2024, 1996, 5628, 1997, 3698, 4083, 1029, 2129, 2000,\n",
            "        4553,  102]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Difference between tokenizer and tokenizer.tokenize"
      ],
      "metadata": {
        "id": "ERT7x3-Jfi8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "sequence = \"What are the branches of Machine Learning? How to learn\"\n",
        "\n",
        "model_input1 = tokenizer(sequence)\n",
        "print(f\"model_input1: {model_input1}\")\n",
        "\n",
        "model_input2 = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(model_input2)\n",
        "print(f\"model_input2: {ids}\")\n",
        "\n",
        "print(tokenizer.decode(model_input1['input_ids']))\n",
        "print(tokenizer.decode(ids))\n",
        "\n",
        "model_input3 = tokenizer(sequence, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(f\"model_input3: {model_input3}\")\n",
        "output = model(**model_input3)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "SvRNHGAyft40",
        "outputId": "b8e6b71e-33ad-49f4-a06d-ace6bedaee9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_input1: {'input_ids': [101, 2054, 2024, 1996, 5628, 1997, 3698, 4083, 1029, 2129, 2000, 4553, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "model_input2: [2054, 2024, 1996, 5628, 1997, 3698, 4083, 1029, 2129, 2000, 4553]\n",
            "[CLS] what are the branches of machine learning? how to learn [SEP]\n",
            "what are the branches of machine learning? how to learn\n",
            "model_input3: {'input_ids': tensor([[ 101, 2054, 2024, 1996, 5628, 1997, 3698, 4083, 1029, 2129, 2000, 4553,\n",
            "          102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.3374, -1.1729]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
          ]
        }
      ]
    }
  ]
}